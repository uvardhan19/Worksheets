DEEP LEARNING – WORKSHEET 5
Q1 to Q8 are MCQs with only one correct answer. Choose the correct option.
1. Which of the following are advantages of batch normalization?
A) Reduces internal covariant shift.
B) Regularizes the model and reduces the need for dropout, photometric distortions, local response normalization and other regularization techniques.
C) allows use of saturating nonlinearities and higher learning rates.
D) All of the above
Ans: D
   2. Which of the following is not a problem with sigmoid activation function? A) Sigmoids do not saturate and hence have faster convergence
B) Sigmoids have slow convergence.
C) Sigmoids saturate and kill gradients.
D) Sigmoids are not zero centered; gradient updates go too far in different directions, making optimization more difficult.
Ans: A
      3. Which of the following is not an activation function?
A) Swish B) Maxout
C) SoftPlus D) None of the above
Ans: D
         4. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. True/False?
A) True B) False
Ans: A
            5. In which of the weights initialisation techniques, does the variance remains same with each passing layer?
A) Bias initialisation B) Xavier Initialisation
C) He Normal Initialisation D) None of these
Ans: B
               6. Which of the following is main weakness of AdaGrad?
A) learning rate shrinks and becomes infinitesimally small B) learning rate doesn’t shrink beyond a point
C) change in learning rate is not adaptive
D) AdaGrad adapts updates to each individual parameter
Ans: D
                  7. In order to achieve right convergence faster, which of the following criteria is most suitable? 
A) momentum and learning rate both must be high
B) momentum must be high and learning rate must be low
C) momentum and learning rate both must be low
D) momentum must be low and learning rate must be high
Ans: C
                  8. When is an error landscape is said to be poor(ill) conditioned?
A) when it has many local minima
B) when it has many local maxima
C) when it has many saddle points and flat areas D) None of these
Ans: D
Q9 and Q10 are MCQs with one or more correct answers. Choose all the correct options.
                     9. Which of the following Gradient Descent algorithms are adaptive?
A) ADAM B) SGD
C) NADAM D) RMS Prop.
Ans: B & D
                        10. When should an optimization function (gradient descent algorithm) stop training:
A) when it reaches local minimum B) when it reaches saddle point
C) when it reaches global minimum
D) when it reaches a local minima which is similar to global minima (i.e. which has very less error distance with global minima)
Ans: C & D
Q11 to Q15 are subjective answer type question. Answer them briefly.
11. What are convex, non-convex optimization?
Ans: A convex optimization problem is a problem where all of the constraints are convex functions, and the objective is a convex function if minimizing, or a concave function if maximizing.  Linear functions are convex, so linear programming problems are convex problems.  Conic optimization problems -- the natural extension of linear programming problems -- are also convex problems. In a convex optimization problem, the feasible region -- the intersection of convex constraint functions -- is a convex region. With a convex objective and a convex feasible region, there can be only one optimal solution, which is globally optimal.  Several methods -- notably Interior Point methods -- will either find the globally optimal solution, or prove that there is no feasible solution to the problem.  Convex problems can be solved efficiently up to very large sizes.
A non-convex optimization problem is any problem where the objective or any of the constraints are non-convex, Such a problem may have multiple feasible regions and multiple locally optimal points within each region.  It can take time exponential in the number of variables and constraints to determine that a non-convex problem is infeasible, that the objective function is unbounded, or that an optimal solution is the "global optimum" across all feasible regions.


12. What do you mean by saddle point? Answer briefly.
Ans: A point at which a function of two variables has partial derivatives equal to zero but at which the function has neither a maximum nor a minimum value. Surfaces can also have saddle points, which the second derivative test can sometimes be used to identify. Examples of surfaces with a saddle point include the handkerchief surface and monkey saddle.
13. What is the main difference between classical momentum and Nesterov momentum? Explain briefly. 
Ans: The main difference is in classical momentum you first correct your velocity and then make a big step according to that velocity (and then repeat), but in Nesterov momentum you first making a step into velocity direction and then make a correction to a velocity vector based on new location (then repeat).
14. What is Pre initialisation of weights? Explain briefly.
Ans: In deep learning, the goal is to find the optimum weights of the model to get the desired output. In transfer learning, the network is initialized using the best pre-trained weights. One of the ways is to initialize all weights to 0s. As all the weights are the same, the activations in all hidden units are also the same. This makes the gradient w.r.t to each weight be the same. Thus, the problem arises as to which weight the network should update and by how much. i.e. backpropagation finds it difficult to minimize the loss. The same problem occurs if all weights are initialized as 1s.
15. What is internal covariance shift in Neural Networks?
Ans: We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on.